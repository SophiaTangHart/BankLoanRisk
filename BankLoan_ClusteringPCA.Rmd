---
title: "<center>Implementing Clustering and PCA Algorithms</center>"
subtitle: "<center>Using Bank Loan Dataset</center>"
date: "<center>`r format(Sys.time(), '%B %Y')`</center>"
author: "<center>Sophia Tang Hart</center>"
output: # html_notebook
  html_document:
      theme: journal
      toc: yes
      toc_depth: 4
      #toc_float: true
  word_document:
      toc: yes
      toc_depth: 4
      #toc_float: true
  pdf_document:
      toc: yes
      theme: journal
      toc_depth: 4
      #toc_float: true
---


# OBJECTIVE AND DATA DESCRIPTION
**Background: **

Banks loan out money to customers to finance a car, a house, pay for education, consolidate loans, etc. Borrowers agree to pay back the money with an interest on a monthly basis. Sometimes, due to unexpected circumstances, some borrowers are not able to pay back the money. 


**Research Objectives: **

The banks would want to see what is the pattern in the customers to predict if a customer can pay back the loan, so the bank knows who to lend out the money. I would like to predict if any customer goes to a bank, should the bank loan out the money to the customer base on model learned from this dataset. 


**Research Questions: ** 

1. What factors contribute/correlated most to bank loan status?

2. Can we predict if a borrower will be able to pay the debt in full?

3. What Machine Learning algorithms perform best in the prediction? (First Guess)

4. Optimize all (or the best) algorithms. With the fine tuned hyperparameters, what is the best prediction performance? Which algorithm?


**Dataset**

The dataset is taken from Kaggle (https://www.kaggle.com/zaurbegiev/my-dataset). There are over 100,000 rows and 19 columns (features) in this dataset. The predicted feature variable is Loan_Status, which is a categorical variable with value either "Fully Paid" or "Charged off". Fully Paid means the borrower can pay back the debt, while charged off means the borrower is unlikely pay the bank after a substantial delinquent for a period of time. The remainder of the debt is sometimes collected by a third-party agency.
 

LoanID,

CustomerID,

Loan_Status,

Current_Loan_Amount,

Term,

Credit_Score,

Annual_Income,

Years_in_current_job,

Home_Ownership,

Purpose,

Monthly_Debt,

Years_of_Credit_History,

Months_since_last_delinquent,

Number_of_Open_Accounts,

Number_of_Credit_Problems,

Current_Credit_Balance,

Maximum_Open_Credit,

Bankruptcies,

Tax_Liens


# STEP 1: LOADING LIBRARIES AND DATA
```{r message=FALSE, warning=FALSE}
library(caret)
library(dplyr)
library(tidyverse)
library(mlbench)
library(gmodels)
# for multiple plots in one figure (ggarrange)
library(ggpubr)
library(ggplot2)
library(clustertend)
library(factoextra)
library(corrplot)
library(cluster)
library(magrittr)
library(fpc)
library(pheatmap)
library(clValid)

theme_set(theme_classic())

```

## Examing data
```{r message=FALSE, warning=FALSE}

# Clear memory
rm(list=ls())

# read in data
BankLoan_dataset <- read.csv("LoanStatus.csv")

# remove unrelevant fields
BankLoan_dataset <- subset(BankLoan_dataset, select=-c(LoanID, CustomerID))

# data structure
knitr::kable(str(BankLoan_dataset))

# data summary
knitr::kable(head(BankLoan_dataset))
knitr::kable(summary(BankLoan_dataset))
```

# STEP 2: DATA PREPROCESSING

## a) Convert to numeric values
```{r message=FALSE, warning=FALSE}

BankLoan_dataset$Years_in_current_job = extract_numeric(BankLoan_dataset$Years_in_current_job)
BankLoan_dataset$Years_in_current_job <- as.numeric(BankLoan_dataset$Years_in_current_job)

```


## b) Convert Categorical Variables
## c) Remove Outliers 
## d) Remove Null

```{r message=FALSE, warning=FALSE}

# # identify outliers
# outliers <- boxplot(BankLoan_dataset$Annual_Income, plot=FALSE)$out
# # remove outliers
# BankLoan_dataset <- BankLoan_dataset[-which(BankLoan_dataset$Annual_Income %in% outliers), ] 
#
```
We cannot remove outliers for individual feature one at a time, because after removing the outliers, some of the records are removing. The rows will be mismatched when we combine the features together. Therefore, we have to use the pipe function like below.

```{r message=FALSE, warning=FALSE}
# show outliers before removing them
histogram(BankLoan_dataset$Annual_Income)

bankloan <- BankLoan_dataset %>%
  # convert string feature into categorical factors
  mutate_if(is.character, as.factor) %>% 
  
  # remove the nulls
  drop_na() %>%

  ### Remove outliers
  # Annual_Income
  filter(between(Annual_Income, 
                 quantile(Annual_Income, 0.25) - 1.5* IQR(Annual_Income),
                 quantile(Annual_Income, 0.75) + 1.5* IQR(Annual_Income))) %>%
  # Current_Loan_Amount
  filter(between(Current_Loan_Amount, 
                 quantile(Current_Loan_Amount, 0.25) - 1.5* IQR(Current_Loan_Amount),
                 quantile(Current_Loan_Amount, 0.75) + 1.5* IQR(Current_Loan_Amount))) %>%
  # Credit_Score
  filter(between(Credit_Score, 
                 quantile(Credit_Score, 0.25) - 1.5* IQR(Credit_Score),
                 quantile(Credit_Score, 0.75) + 1.5* IQR(Credit_Score))) %>%
  #Number_of_Credit_Problems (there is an outlier of 15)
  filter(between(Number_of_Credit_Problems, 
                 quantile(Number_of_Credit_Problems, 0.25) - 1.5* IQR(Number_of_Credit_Problems),
                 4)) %>%
  # Monthly_Debt
  filter(between(Monthly_Debt, 
               quantile(Monthly_Debt, 0.25) - 1.5* IQR(Monthly_Debt),
               quantile(Monthly_Debt, 0.75) + 1.5* IQR(Monthly_Debt))) %>%

  # Maximum_Open_Credit
  filter(between(Maximum_Open_Credit, 
               quantile(Maximum_Open_Credit, 0.25) - 1.5* IQR(Maximum_Open_Credit),
               quantile(Maximum_Open_Credit, 0.75) + 1.5* IQR(Maximum_Open_Credit))) %>%
    
  #remove null after filling outliers with NA
  drop_na()

knitr::kable(str(bankloan))
knitr::kable(summary(bankloan))

#check if there is null
is.null(bankloan)

# showing no outliers
histogram(bankloan$Annual_Income)

```

Histogram shows outliers are removed. Annual income is skewed right. After removing outliers and Null values, there are still 26,500 data left to work with.


# STEP 3: STATISTICAL SUMMARY

## a) Graphical Summary
```{r message=FALSE, warning=FALSE}
# BAR graph for categorical variables

ggplot(bankloan, aes(x=Purpose)) +
  geom_bar() +
  coord_flip()

gg_status <- ggplot(bankloan, aes(x=Loan_Status)) +
  geom_bar()

gg_home <- ggplot(bankloan, aes(x=Home_Ownership)) +
  geom_bar() +
  coord_flip()

gg_problem <- ggplot(bankloan, aes(x=Number_of_Credit_Problems)) +
  geom_bar() 

gg_job <- ggplot(bankloan, aes(x=Years_in_current_job)) +
  geom_bar() 

# arrange multiple plots in one figure  
figure <- ggarrange(gg_status, gg_home, gg_problem, gg_job,
                    ncol = 2, nrow = 2,
                    legend="none")
figure


# Boxplot for quantitative variables
ggplot(bankloan, aes(x=Loan_Status, y=Annual_Income)) +
  geom_boxplot()

ggplot(bankloan, aes(x=Loan_Status, y=Current_Loan_Amount)) +
  geom_boxplot()

ggplot(bankloan, aes(x=Loan_Status, y=Credit_Score)) +
  geom_boxplot()

ggplot(bankloan, aes(x=Loan_Status, y=Monthly_Debt)) +
  geom_boxplot()

```

From the bar graphs, we can see that the data is imbalanced, Paid Fully is much more than Charge Off. The home_ownership is have_mortgage most and rent is secondly. Most people have zero number of credit problems. Most people work 10+ years in current job. The most purpose of loans is Debt Consolidation.

From the Boxplots, comparing annual income, current loan amount, monthly debt and credit score, annual income seems to be the biggest difference between Fully Paid and Charged Off customers.


## b) Numerical Summary
```{r message=FALSE, warning=FALSE}
# proportion of Paid-fully and Charged-off
table(bankloan$Loan_Status)
round(prop.table(table(bankloan$Loan_Status)) * 100, 1)

# Average group by loan status
# Annual_Income is column5, credit score is column4
aggregate(bankloan[, 4:5], list(bankloan$Loan_Status), median)

```

Numerical statistics shows Fully Paid is 81.8% of the total data. The median annual income for fully paid customers is 1.23M and 1.12M for charged off customers. Since the annual income is right-skew, median is used instead of mean.


# STEP 4: PRINCIPAL COMPONENT ANALYSIS (PCA)

## a) Balancing Data by Downsampling
```{r message=FALSE, warning=FALSE}
set.seed(9650)
# Divide data into train and test sets
indexTrain <- createDataPartition(y=bankloan$Loan_Status, p=0.75, list=FALSE) 
training <- bankloan[indexTrain, ]
testing <- bankloan[-indexTrain, ]

# Since there are 26000 of data, take a subset of data for visual clarity
training <- training[1:2000, ]
testing <- testing[1:500, ]

# Downsampling to balance data
train <- downSample(x=training[, -ncol(training)], y=training$Loan_Status)
test <- downSample(x=testing[, -ncol(testing)], y=testing$Loan_Status)

table(train$Loan_Status)
table(test$Loan_Status)
```
Data are balanced with downsampling. 

## b) Applying PCA with Linear Model

```{r message=FALSE, warning=FALSE}

# Using balanced data. Taking only quantitative variables
trainQ <- train[, c(2,4,5,6,9,10,11,12,14,15)]
testQ <- test[, c(2,4,5,6,9,10,11,12,14,15)]

# Transform data, applying PCA. Taking Annual_Income as the predicted feature
preProc <- preProcess(trainQ[,-3], method=c('BoxCox', 'center', 'scale', 'pca'), Comp=7)
# Applying PCA, predict train data without predicted feature
train_pca <- predict(preProc, trainQ[, -3])
# Adding column that needs to be predicted
train_pca$Annual_Income <- trainQ$Annual_Income
# Examing
head(train_pca)

# Applying PCA, predict test data without predicted feature
test_pca <- predict(preProc, testQ[, -3])
# Adding column that needs to be predicted
test_pca$Annual_Income <- testQ$Annual_Income
head(test_pca)

# Fitting linear model using components
fit <- lm(Annual_Income~., data=train_pca)
print(fit$coefficients)
summary(fit)

# Predicting on test set
predictions <- predict(fit, test_pca)
head(predictions)

# Evaluation, root mean square error
rmse <- RMSE(predictions, testQ$Annual_Income)
rmse
error_rate <- rmse/mean(testQ$Annual_Income)
error_rate
```
All Principal Components except PC5 are statistically significant looking from the p-values. Prediction with PCA gives RMSE of 402142 or 32.4%. The RMSE is large because the data have very large number units. R2 is 32%.  

## c) Visualization of PCA with Evaluation Statistics
### 1. Using Imbalanced Data
```{r message=FALSE, warning=FALSE}

# Taking a subset of quantitative variables for easier showing the concepts. Taking only 300 observation because the data is too big to visualize
bankloan_sub <- bankloan[1:300, c('Annual_Income', 'Monthly_Debt', 'Months_since_last_delinquent', 'Maximum_Open_Credit')] # 'Maximum_Open_Credit', 'Credit_Score', 

Status_sub <- bankloan[1:300, 'Loan_Status']
# Summarizing the data
summary(bankloan_sub)

# Scale the data
bankloan_scaled <- scale(bankloan_sub)
summary(bankloan_scaled)

# Creating and printing Principal Components using function prcomp
# habillage: an optional factor variable for coloring the observations by groups

fviz_pca_ind(prcomp(bankloan_scaled), title='Bank Loan Status Data', habillage=Status_sub, palette='jco', geom='point', ggtheme=theme_classic(), legend='bottom')

# Counting variation captured by each Principal Component
PCA_bankloan <- prcomp(bankloan_scaled)

summary(PCA_bankloan)
print(PCA_bankloan) # same as rotation/direction
PCA_bankloan$center # at the origin
PCA_bankloan$rotation

# Visualization, Counting variation captured by each Principal Component
eigen_exp <- fviz_eig(PCA_bankloan)
eigen_exp

# Visualization, Principal Component as linear combination of each of the variables
eigen_contri1 <- fviz_contrib(PCA_bankloan, choice='var', axes=1)
eigen_contri1
eigen_contri2 <- fviz_contrib(PCA_bankloan, choice='var', axes=2)
eigen_contri2
eigen_contri1_2 <- fviz_contrib(PCA_bankloan, choice='var', axes=1:2)
eigen_contri1_2

# Quantitative
var <- get_pca_var(PCA_bankloan)
head(var$contrib)

```
Looking at the clusters, Charge-Off and Fully-Paid look overlapping. Taking only four predictor features with original unbalanced data, PC1 accounts for 46.3% of data variation, and PC2 accounts for 26.3% of data variation. Monthly_Debt and Annual_Income contributes more than 25% for PC1. Months_since_last_delinquent contributes 80% of PC2.

### 2. Improvement: Using Balanced Data
```{r message=FALSE, warning=FALSE}

# Shuffling data
train <- train[sample(nrow(train)), ]

# Using the downsampled balanced data (train). Taking a subset of quantitative variables for easier showing the concepts. 
bankloan_sub <- train[1:300, c('Annual_Income', 'Monthly_Debt', 'Months_since_last_delinquent', 'Maximum_Open_Credit')] # 'Maximum_Open_Credit', 'Credit_Score', 

Status_sub <- train[1:300, 'Loan_Status']
# Summarizing the data
summary(bankloan_sub)

# Scale the data
bankloan_scaled <- scale(bankloan_sub)
summary(bankloan_scaled)

# Creating and printing Principal Components using function prcomp
# habillage: an optional factor variable for coloring the observations by groups

fviz_pca_ind(prcomp(bankloan_scaled), title='Bank Loan Status Data', habillage=Status_sub, palette='jco', geom='point', ggtheme=theme_classic(), legend='bottom')

# Counting variation captured by each Principal Component
PCA_bankloan <- prcomp(bankloan_scaled)

summary(PCA_bankloan)
print(PCA_bankloan) # same as rotation/direction
PCA_bankloan$center # at the origin
PCA_bankloan$rotation

# Visualization, Counting variation captured by each Principal Component
eigen_exp <- fviz_eig(PCA_bankloan)
eigen_exp

# Visualization, Principal Component as linear combination of each of the variables
eigen_contri1 <- fviz_contrib(PCA_bankloan, choice='var', axes=1)
eigen_contri1
eigen_contri2 <- fviz_contrib(PCA_bankloan, choice='var', axes=2)
eigen_contri2
eigen_contri1_2 <- fviz_contrib(PCA_bankloan, choice='var', axes=1:2)
eigen_contri1_2

# Quantitative
var <- get_pca_var(PCA_bankloan)
head(var$contrib)


```
With the balanced data, Charge-Off and Fully-Paid still look overlapping from the cluster.  PC1 accounts for 45% of data variation, and PC2 accounts for 25.4% of data variation. This is slightly less than the variation accounted with the original unbalanced data. This is surprising to me. Monthly_Debt and Annual_Income contributes more than 25% for PC1.

### 3. Improvement: Using All Features
```{r message=FALSE, warning=FALSE}

# Using balanced data. Taking all quantitative variables
bankloan_sub <- train[1:300, c(2,4,5,6,9,10,11,12,14,15)]

Status_sub <- train[1:300, 'Loan_Status']
# Summarizing the data
summary(bankloan_sub)

# Scale the data
bankloan_scaled <- scale(bankloan_sub)
summary(bankloan_scaled)

# Creating and printing Principal Components using function prcomp
# habillage: an optional factor variable for coloring the observations by groups

fviz_pca_ind(prcomp(bankloan_scaled), title='Bank Loan Status Data', habillage=Status_sub, palette='jco', geom='point', ggtheme=theme_classic(), legend='bottom')

# Counting variation captured by each Principal Component
PCA_bankloan <- prcomp(bankloan_scaled)

summary(PCA_bankloan)
print(PCA_bankloan) # same as rotation/direction
PCA_bankloan$center # at the origin
PCA_bankloan$rotation

# Visualization, Counting variation captured by each Principal Component
eigen_exp <- fviz_eig(PCA_bankloan)
eigen_exp

# Visualization, Principal Component as linear combination of each of the variables
eigen_contri1 <- fviz_contrib(PCA_bankloan, choice='var', axes=1)
eigen_contri1
eigen_contri2 <- fviz_contrib(PCA_bankloan, choice='var', axes=2)
eigen_contri2
eigen_contri1_2 <- fviz_contrib(PCA_bankloan, choice='var', axes=1:2)
eigen_contri1_2

# Quantitative
var <- get_pca_var(PCA_bankloan)
head(var$contrib)


```
With all quantitative variables, we need to include PC1-PC6 in order to account for >80% variation.

# STEP 5: K-MEANS VISUALIZATION WITH EVALUATION STATISTICS
```{r message=FALSE, warning=FALSE}

# Using balanced data
# Taking only 300 observation because the data is too big to visualize
bankloan_sub <- train[1:300, c('Annual_Income', 'Monthly_Debt', 'Months_since_last_delinquent', 'Maximum_Open_Credit')] # 'Maximum_Open_Credit', 'Credit_Score', 

# Scale the data
bankloan_scaled <- scale(bankloan_sub)
#summary(bankloan_scaled)

# Elbow Method
elbow_figure <- fviz_nbclust(bankloan_scaled, kmeans, method='wss')
elbow_figure

# Silhouette Score
sil <- fviz_nbclust(bankloan_scaled, kmeans, method='silhouette')
sil

# Gap Statistics
gap_stat <- clusGap(bankloan_scaled, FUN=kmeans, nstart=25, K.max=10, B=50)
fviz_gap_stat(gap_stat)

# Applying kMeans
kmeans_clust <- kmeans(bankloan_scaled, 2)

# Visualize Cluster Plot
fviz_cluster(list(data=bankloan_scaled, cluster=kmeans_clust$cluster), ellipse.type='norm', geom='point', stand=FALSE, palette='jco')

# Calculating hopkins statistics which show if the data exhibit inherent patterns
print(hopkins(bankloan_scaled, n=nrow(bankloan_scaled)-1))

# Visualizing the dissimilarity matrix
# Pink/red is high in similarity, blue/purple is low in similarity.
fviz_dist(dist(bankloan_scaled), show_labels = FALSE) +
  labs(title='Bank Loan Status Dataset')
```

The Hopkins Statistics is 0.37. This indicates data do not have good separability, but rather random. This agrees with the PCA clustering in the previous section. However, supervised algorithms like k-Nearest Neighbors and Random Forest give 98%-100% accuracy. This is probably this data work well for supervised algorithms but not so great on unsupervised algorithms like PCA and k-Means. Also categorical variables were included in the supervised algorithms. 

In terms of predicting number of clusters, both Elbow Method and Silhouhette Score predict 2 is optimal, while Gap predicted 1, which means all data are the same group and this would not make sense. 

# STEP 6: HIERARCHICAL CLUSTERING

## a) Using Ward Linkage Function
```{r message=FALSE, warning=FALSE}

# Taking a subset of quantitative variables for easier showing the concepts. Taking only 300 observation of the balanced data because the data is too big to visualize
bankloan_sub <- train[1:300, c('Annual_Income', 'Monthly_Debt', 'Months_since_last_delinquent', 'Maximum_Open_Credit')] # 'Maximum_Open_Credit', 'Credit_Score', 

# Scale the data
bankloan_scaled <- scale(bankloan_sub)
#summary(bankloan_scaled)

# Creating dissimilarity matrix suing Euclidean distance
bankloan_dist <- dist(bankloan_scaled, method='euclidean')

# Linkage function utilizes the distance as a proximity metric and pair wise merges the instances thereby creating larger clusters with every successive iteration. Using linkage function ward 2 that creates clusters by minimizing variance.
# https://en.wikipedia.org/wiki/Ward%27s_method

agg_tree_ward <- hclust(d=bankloan_dist, method='ward.D2')
print(agg_tree_ward)

# Visualizing Dendrogram
fviz_dend(agg_tree_ward, cex=.5)

# Cutting tree to create 2 clusters and visualizing it
agg_tree_ward_dend <- fviz_dend(agg_tree_ward, cex=.5, k=2, palette='jco')
agg_tree_ward_dend

# To access the partition accuracy of the cluster tree (created by hclust()) there should be a strong correlation between the original distance matrix and the object linkage distance defined as Cophenetic Distances. 
# Cophenetic correlation coefficient measures of how faithfully a dendogram preserves the pairwise distances between the original data points.

agg_cophenetic <- cophenetic(agg_tree_ward)

cor(bankloan_dist, agg_cophenetic)
```
Using the Ward Linkage function, I get Cophenetic correlation coefficient of 0.5. I used balanced data for this, but the cluster Dendrogram show one cluster is twice as big as the other.

## b) Improvement: Using Average Linkage Function
```{r message=FALSE, warning=FALSE}

agg_tree_avg <- hclust(d=bankloan_dist, method='average')
print(agg_tree_avg)

# Visualizing Dendrogram
fviz_dend(agg_tree_avg, cex=.5)

# Cutting tree to create 2 clusters and visualizing it
agg_tree_avg_dend <- fviz_dend(agg_tree_avg, cex=.5, k=2, palette='jco')
agg_tree_avg_dend

# Cophenetic Distance
agg_cophenetic <- cophenetic(agg_tree_avg)
# Correlation between Cophenetic distances and original distances
cor(bankloan_dist, agg_cophenetic)

# Cutting tree into two clusters and Visualizing it
two_groups <- cutree(agg_tree_avg, k=2)
fviz_cluster(list(data=bankloan_scaled, cluster=two_groups))
```

Using the Average Linkage function, I get Cophenetic correlation coefficient of 0.65, which is better than the Ward Linkage function. However, the dendrogram and cluster plot show that one cluster is much larger than the other. This is not true because I was using balanced data. Therefore, Average Linkage function does not work well here. 

## c) Improvement: Using Complete Linkage Function
```{r message=FALSE, warning=FALSE}

# Using the Complete (i.e. Maximum) linkage 
agg_tree_max <- hclust(d=bankloan_dist, method='complete')
print(agg_tree_max)

# Visualizing Dendrogram
fviz_dend(agg_tree_max, cex=.5)

# Cutting tree to create 2 clusters and visualizing it
agg_tree_max_dend <- fviz_dend(agg_tree_max, cex=.5, k=2, palette='jco')
agg_tree_max_dend

# Cophenetic Distance
agg_cophenetic <- cophenetic(agg_tree_max)
# Correlation between Cophenetic distances and original distances
cor(bankloan_dist, agg_cophenetic)

```

Similar to Average Linkage, using Complete/Maximum Linkage function, the dendrogram shows that one cluster is much larger than the other. This is not true because I was using balanced data. Therefore, the Ward Linkage function works best for my data. 

## d) Using the Cluster package for Agglomerative and Divisive Methods
```{r message=FALSE, warning=FALSE}

# Agglomerrative (Bottom-up approach)
agnes_cluster <- agnes(x=bankloan_scaled, stand=TRUE, metric='euclidean', method='average')
# Plune tree
#agnes_tree <- pltree(agnes_cluster, cex=.5, hang=-1, main='Dendrogram of Agnes')
fviz_dend(agnes_cluster, cex=.6, k=2, main='Dendrogram of Agnes')

# Divisive (Top-down approach). Divisive approach does not need linkage function
diana_cluster <- diana(x=bankloan_scaled, stand=TRUE, metric='euclidean')
fviz_dend(diana_cluster, cex=.6, k=2, main='Dendrogram of Diana')

```

Using Cluster Package, Divisive (top-down) gives better cluster than Agglomerrative (bottom-up) method by looking at the size balance of the two cluster. 

## e) Heatmap
```{r message=FALSE, warning=FALSE}

library(pheatmap)

knitr::kable(str(bankloan_scaled))
knitr::kable(summary(bankloan_scaled))

# Heatmaps are used for Visualizing Hierarchical clustering.
# Heat Maps are used to visualize clusters of samples and features. The high values are in red and low in blue.
# cellheight can be set to a higher number to see the details of each row (such as representing a gene)
pheatmap(bankloan_scaled, cutree_rows = 2, cellheight = .4)

```

# STEP 7: COMPARING CLUSTERING ALGORITHMS
clValid reports validation measures for clustering results. The function returns an object of class "'>clValid", which contains the clustering results in addition to the validation measures. The validation measures fall into three general categories: "internal", "stability", and "biological".

## a) Internal Validation Measures
```{r message=FALSE, warning=FALSE}
library(clValid)

bankloan_sub <- train[1:300, c('Annual_Income', 'Monthly_Debt', 'Months_since_last_delinquent', 'Maximum_Open_Credit')] # 'Maximum_Open_Credit', 'Credit_Score', 

# Scale the data
bankloan_scaled <- scale(bankloan_sub)
knitr::kable(summary(bankloan_scaled))

# Comparing Hierarchical, k-Means and PAM (Partitioning Around Mediods -- medians) algorithms
clmethods <- c('hierarchical', 'kmeans', 'pam')

# Using 'internal' as evaluation metric
evaluation <- clValid(bankloan_scaled, nClust=2:6, clMethods=clmethods, validation='internal')

summary(evaluation)
optimalScores(evaluation)

# Visualization
plot(evaluation)
```

Comparing k-Means, Hierarchical, and PAM (Partitioning Around Mediods -- medians) algorithms, Hierarchical clustering gives the best Internal validation measures in terms of connectivity (want minimum), Dunn index (want as close to 1) and Silhouette Score (want as close to 1).

## b) Stability Measures
```{r message=FALSE, warning=FALSE}

# Using 'stability' as validation measure
evaluation <- clValid(bankloan_scaled, nClust=2:6, clMethods=clmethods, validation='stability')

summary(evaluation)
optimalScores(evaluation)

# Visualization
plot(evaluation)

```

Comparing k-Means, Hierarchical, and PAM (Partitioning Around Mediods -- medians) algorithms, Hierarchical gives the best stability measures in terms of APN and ADN. It predicts 2 cluster is the optimal number. k-Means gives the best stability in terms of AD and FOM. It predicts 6 clusters is the optimal number. So Hierarchical is a better prediction, since I know the data has only two classes.


# STEP 8: HYPOTHESIS TESTING FOR K-NN AND RANDOM FOREST

## a) k-Nearest Neighbors Algorithm
```{r message=FALSE, warning=FALSE}

set.seed(9650)

# Divide data into train and test sets
indexTrain <- createDataPartition(y=bankloan$Loan_Status, p=0.75, list=FALSE) 
training <- bankloan[indexTrain, ]
testing <- bankloan[-indexTrain, ]

table(training$Loan_Status)

# ROSE resampling to balance data
down_train <- downSample(x=training[, -ncol(training)], y=training$Loan_Status)
down_test <- downSample(x=testing[, -ncol(testing)], y=testing$Loan_Status)

table(down_train$Loan_Status)
table(down_test$Loan_Status)

# cross validation and fitting model
fitControl <- trainControl(method='repeatedcv', number=10)
down_kNNfit <- train(Loan_Status~., data=down_train, method='knn', trControl=fitControl, preProcess=c('BoxCox', "center", "scale"), tuneLength=12)

#print output
down_kNNfit
plot(down_kNNfit)

# Prediction
down_kNNpredict <- predict(down_kNNfit, newdata=down_test)

# confusion matrix to see accuracy and other parameter values
confusionMatrix(down_kNNpredict, down_test$Loan_Status)

# calculate accuracy
mean(down_kNNpredict == down_test$Loan_Status)
```

## b) Random Forest Algorithm

```{r message=FALSE, warning=FALSE}

set.seed(9650)

# Number of variables randomly sampled as candidates at each split.
mtry = sqrt(ncol(training))

# setting the mtry value
tunegrid <- expand.grid(mtry=mtry)

# Cross Validation for parameter tuning
fitControl <- trainControl(method='repeatedcv', number=10, repeats=1)

# train the model
RFfit_down <- train(Loan_Status~., data=down_train, method='rf', ntree=100, tunegrid=tunegrid, trControl=fitControl, preProcess=c('BoxCox', "center", "scale"))

#print output
RFfit_down
plot(RFfit_down)

# Prediction
RFpredict_down <- predict(RFfit_down, newdata=down_test)

# confusion matrix to see accuracy and other parameter values
confusionMatrix(RFpredict_down, down_test$Loan_Status)

# compute accuracy by comparing prediction with actual classification
mean(RFpredict_down == down_test$Loan_Status)


```


## c) T-test Comparing kNN and RF

```{r message=FALSE, warning=FALSE}

set.seed(9650)

# Original data is factor or integers. Have to convert it into numeric or double for t-test comparison to work
kNNpredict <- as.numeric(down_kNNpredict)
RFpredict <- as.numeric(RFpredict_down)
typeof(kNNpredict)
typeof(RFpredict_down)

summary(kNNpredict)
summary(RFpredict_down)

# Paired t-test is applied, because they are the same observations and analyzed using different algorithms.
# alternative='less' propose that kNN is less accurate than Random Forest
# Testing the prediction
t.test(kNNpredict, RFpredict, alternative = 'less', paired = TRUE, conf.level = 0.95)

# alternative = 'two.sided', propose that two algorithms are different
t.test(kNNpredict, RFpredict, alternative = 'two.sided', paired = TRUE, conf.level = 0.95)



# Testing the model
#t.test(down_kNNfit, RFfit_down, alternative = 'less', paired = TRUE, conf.level = 0.95)

summary(down_kNNfit)
summary(RFfit_down)
# Can not compare two models because they have different parameters.

```
This section is a hypothesis testing to see if k-NN and Random Forest the different performance is statically significant or is it by chance. 

p-value = 0.9 (>0.05) for "k-NN has lower performance than RF". Therefore, data do not provide enough evidence that k-NN has lower performance than RF. 

p-value = 0.19 (>0.05) for "k-NN has different performance than RF". Therefore, data do not provide enough evidence that k-NN has different performance than RF.

Taken together, the difference of k-NN performance accuracy of 98% and RF performance accuracy of 100% is by chance and not stastically significant.  


# STEP 9: CONCLUSION

Unsupervised algorithms are more challenging than supervised algorithm because there is no target feature to train the model. That is the reason k-NN and Random Forest gives 98%-100% accuracy while PCA, k-Means and Hierarchical Clustering gives much less accurate predictions on this bank loan status dataset.

Nonetheless, Principal Components PC1 and PC2 combined accounts for 71% variability in the data. Hierarchical clustering gives better internal validation and stability measures than k-Means and PAM. Of the Hierarchical clustering, Divisive method gives better result than agglomerrative approach in terms of balance of the two cluster sizes given balanced data were used in the model. Silhouette score and Elbow method both predict 2 clusters, while Gap predicted 1 wrongly. 

Hypothesis testing shows that the difference of k-NN performance accuracy of 98% and RF performance accuracy of 100% is by chance and the difference is not statically significant. This is interesting to know.

A weakness of this dataset is that the two clusters do not have good separability (Hopkins Statistics is 0.37). This is probably because the data is noisy or data is not ideal for unsupervised algorithms. However, I cannot change the inherent nature/pattern of the data. So PAC, k-Means and Hierarchical clustering analyses were carried out even though the data is not ideal.  

In the future, it would be helpful to find or develop an unsupervised algorithm that can improve the prediction result, because the prediction was excellent when providing with a target variable.